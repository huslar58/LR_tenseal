{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plaintext linear regression\n",
    "\n",
    "The plaintext and encrypted linear regression are implemented to simulate the exemplary use case one in the main report.\n",
    "\n",
    "In this Jupyter playbook, the plaintext linear regression model is implemented by using the dataset Apartment rental offers in Germany from kaggle (https://www.kaggle.com/datasets/corrieaar/apartment-rental-offers-in-germany). The code is based on the code from the module Advanced Machine Learning Exercises, made by Solange Emmenegger (Solange Emmenegger, Hochschule Luzern, Module Advanced Machine Learning, accessed on 19 April 2024 at https://gitlab.renku.hslu.ch/solange.emmenegger/ml-adml-hslu/-/tree/master/notebooks/03A%20Supervised%20Learning, and https://gitlab.renku.hslu.ch/solange.emmenegger/ml-adml-hslu/-/blob/master/notebooks/04B%20Gradient%20Descent/Gradient%20Descent.ipynb) and modified where necessary. \n",
    "\n",
    "Firstly, the libraries are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from time import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "import psutil\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "In this section, the dataset is prepared for processing. The following actions are performed:\n",
    "\n",
    "- Amount of features reduced\n",
    "- Data converted to appropriate datatypes\n",
    "- Unsensical data is removed\n",
    "- NA values are removed\n",
    "- Outliers are removed as linear regression is sensitive to outliers\n",
    "- Categorical data is converted to numeric data\n",
    "- The data is split into a train and test set\n",
    "- The data is scaled as linear regression is sensitive to data ranges.\n",
    "\n",
    "The code leans on the exercises from the module ADML (https://gitlab.renku.hslu.ch/solange.emmenegger/ml-adml-hslu/-/tree/master/notebooks/03A%20Supervised%20Learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"immo_data.csv\")\n",
    "\n",
    "columns_to_drop = ['serviceCharge', 'telekomTvOffer', 'noParkSpaces', 'pricetrend','regio3', 'heatingType', 'telekomUploadSpeed', 'telekomHybridUploadSpeed', 'lastRefurbish', 'newlyConst', 'picturecount', 'firingTypes', 'hasKitchen', 'geo_bln','cellar', 'yearConstructedRange', 'baseRent', 'houseNumber', 'geo_krs', 'interiorQual', 'petsAllowed', 'street', 'streetPlain', 'baseRentRange', 'geo_plz', 'thermalChar', 'floor', 'numberOfFloors', 'noRoomsRange', 'garden', 'livingSpaceRange', 'description', 'facilities', 'heatingCosts', 'energyEfficiencyClass', 'electricityBasePrice', 'electricityKwhPrice', 'date' ]\n",
    "df = df.drop(columns=columns_to_drop, axis=1)\n",
    "df['regio1'] = df.regio1.astype('category')\n",
    "df['regio2'] = df.regio1.astype('category')\n",
    "df['balcony'] = df['balcony'].astype(int)\n",
    "df['lift'] = df['lift'].astype(int)\n",
    "df['condition'] = df.condition.astype('category')\n",
    "df['typeOfFlat'] = df.typeOfFlat.astype('category')\n",
    "df = df.dropna()\n",
    "df = df[(df['totalRent'] != 0) & (df['livingSpace'] != 0) & (df['yearConstructed'] > 1940) & (df['yearConstructed'] < 2021)]\n",
    "numerical_cols = ['totalRent', 'yearConstructed', 'livingSpace', 'noRooms']\n",
    "# Remove outliers\n",
    "q3 = df.loc[:, numerical_cols].describe().loc['75%']\n",
    "iqr = q3 - df.loc[:, numerical_cols].describe().loc['25%']\n",
    "upper_boundary = q3 + 1.5*iqr\n",
    "upper_boundary\n",
    "\n",
    "df = df[(df.totalRent <= upper_boundary.totalRent) &\n",
    "        (df.yearConstructed <= upper_boundary.yearConstructed) &\n",
    "        (df.livingSpace <= upper_boundary.livingSpace) &\n",
    "         (df.noRooms <= upper_boundary.noRooms) ]\n",
    "\n",
    "df = pd.concat([df, pd.get_dummies(df.regio1)], axis='columns')\n",
    "df = pd.concat([df, pd.get_dummies(df.regio2)], axis='columns')\n",
    "df = pd.concat([df, pd.get_dummies(df.condition)], axis='columns')\n",
    "df = pd.concat([df, pd.get_dummies(df.typeOfFlat)], axis='columns')\n",
    "df.drop('regio1', axis='columns', inplace=True)\n",
    "df.drop('regio2', axis='columns', inplace=True)\n",
    "df.drop('condition', axis='columns', inplace=True)\n",
    "df.drop('typeOfFlat', axis='columns', inplace=True)\n",
    "df.drop(['scoutId'], axis='columns', inplace=True)\n",
    "\n",
    "train_rents, test_rents = train_test_split(df, test_size=0.4, random_state=42, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "train_rents = pd.DataFrame(scaler.fit_transform(train_rents), columns=train_rents.columns, index=train_rents.index)\n",
    "test_rents = pd.DataFrame(scaler.transform(test_rents), columns=test_rents.columns, index=test_rents.index)\n",
    "X_train_rents = train_rents.drop(columns=[\"totalRent\"]).values\n",
    "X_test_rents = test_rents.drop(columns=[\"totalRent\"]).values\n",
    "y_train_rents = train_rents.totalRent.values\n",
    "y_test_rents = test_rents.totalRent.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "\n",
    "The following code implements the cost function: $$J = \\frac{1}{N} \\sum_{j=1}^N (y_j - \\hat{y}_j)^2$$\n",
    "\n",
    "The 2 * N is used due to optimizations beyond the scope of this project. Please refer to the lectures of the ADML module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(y, y_pred):\n",
    "    cost = np.sum(np.square(y - y_pred))/ (2 * len(y))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict function\n",
    "\n",
    "The predict function implements the equation of the linear regression in arbitrary dimensions: $$\\check{Y} = \\theta X$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, bias, thetas):\n",
    "    y_pred = bias + np.dot(X, thetas)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient function \n",
    "\n",
    "The gradient function calculates the gradient at a given bias and theta. According to theory, the equation is the following:\n",
    "\n",
    "$$dJ(\\theta) = -2X^T y + 2X^T X\\theta$$\n",
    "\n",
    "However, the code implements the gradient in a different way, with the same result. This follows the approach at the ADML module at Hochschule Luzern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, y, bias, thetas):\n",
    "    diff = predict(X, bias, thetas) - y\n",
    "    n = len(X)\n",
    "    grad_bias = np.sum(diff) / n\n",
    "    grad_thetas = np.dot(diff, X) / n\n",
    "    \n",
    "    return grad_bias, grad_thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit function\n",
    "\n",
    "The fit function optimizes the parameters theta and the bias to the training data. The data is fitted with a mini batch approach, therefore mini batches of samples are fed into the algorithm. As with other code, this code is based on the exercises at the ADML module (https://gitlab.renku.hslu.ch/solange.emmenegger/ml-adml-hslu/-/blob/master/notebooks/04B%20Gradient%20Descent/Gradient%20Descent.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X_train, y_train, alpha, num_epochs, batch_size, display_every=50):\n",
    "    bias = 0.0\n",
    "    thetas = np.random.randn(*(1, X_train.shape[1])).reshape(-1)\n",
    "    hist = defaultdict(list)\n",
    "    \n",
    "    indices_train = np.arange(len(X_train))   \n",
    "    \n",
    "    num_samples = len(X_train)\n",
    "    steps = int(num_samples/batch_size)\n",
    "    \n",
    "    for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "        # Shuffle inputs\n",
    "        np.random.shuffle(indices_train)\n",
    "        \n",
    "        X_train_epoch = X_train[indices_train]\n",
    "        y_train_epoch = y_train[indices_train]\n",
    "        \n",
    "        # Create minibatches and feed them into the algorithm\n",
    "        for step in range(steps):\n",
    "            start = step * batch_size\n",
    "            end = step * batch_size + batch_size\n",
    "            \n",
    "            X_train_mini = X_train_epoch[start:end]\n",
    "            y_train_mini = y_train_epoch[start:end]\n",
    "        \n",
    "            # Calculate the gradients\n",
    "            grad_bias, grad_thetas = gradient(X_train_mini, y_train_mini, bias, thetas)\n",
    "            # Update the parameters\n",
    "            bias = bias - alpha * grad_bias\n",
    "            thetas = thetas - alpha * grad_thetas\n",
    "\n",
    "        y_pred_train = predict(X_train, bias, thetas)\n",
    "        \n",
    "        train_cost = cost(y_train, y_pred_train)\n",
    "        train_r2 = r2_score(y_train, y_pred_train)\n",
    "\n",
    "        hist[\"train_cost\"].append(train_cost)\n",
    "        hist[\"train_r2\"].append(train_r2)\n",
    "        \n",
    "        if epoch % display_every == 0 or epoch == num_epochs:\n",
    "            print(\"Epoch {0} - train_cost: {1:.2} - train_r2: {2:.4}\".format(epoch, train_cost, train_r2))\n",
    "        \n",
    "    return bias, thetas, hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, the parameters of the training are defined. \n",
    "\n",
    "- len_taken is the sliced dataset for comparison. The encrypted linear regression takes significantly more time, therefore, it was decided to cut the dataset to avoid days of training\n",
    "- batch_size is how many samples are taken at once to train\n",
    "- alpha defines the size of the gradient descent step\n",
    "- num_epochs defines for how many epochs the linear regression will be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f3b59349ce4b2b8543dd124dfba863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - train_cost: 0.13 - train_r2: 0.7433\n",
      "Training of the plaintext linear regression took 0 seconds\n",
      "Memory usage difference: 0 bytes\n"
     ]
    }
   ],
   "source": [
    "len_taken = 10000\n",
    "batch_size = 100\n",
    "\n",
    "alpha = 0.01\n",
    "num_epochs = 6\n",
    "\n",
    "mem_usage = psutil.Process().memory_info().rss\n",
    "t_start = time()\n",
    "bias, thetas, hist_rent = fit(X_train_rents[:len_taken], y_train_rents[:len_taken], alpha, num_epochs, batch_size)\n",
    "mem_usage_end = psutil.Process().memory_info().rss\n",
    "t_end = time()\n",
    "print(f\"Training of the plaintext linear regression took {int(t_end - t_start)} seconds\")\n",
    "\n",
    "# Calculate the differences\n",
    "mem_diff = mem_usage_end - mem_usage\n",
    "t_end = time()\n",
    "print(f\"Memory usage difference: {mem_diff} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the plain linear regression is validated on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.7369907022562354\n"
     ]
    }
   ],
   "source": [
    "y_pred_rents_enc = predict(X_test_rents, bias, thetas)\n",
    "r2 = r2_score(y_test_rents, y_pred_rents_enc)\n",
    "print(\"R2:\", r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
