{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With CKKS encrypted linear regression\n",
    "\n",
    "The encrypted linear regression follows the code from the plain linear regression, which is based on the exercises of the ADML module by Solange Emmenegger (Solange Emmenegger, Hochschule Luzern, Module Advanced Machine Learning, accessed on 19 April 2024 at https://gitlab.renku.hslu.ch/solange.emmenegger/ml-adml-hslu/-/tree/master/notebooks/03A%20Supervised%20Learning, and https://gitlab.renku.hslu.ch/solange.emmenegger/ml-adml-hslu/-/blob/master/notebooks/04B%20Gradient%20Descent/Gradient%20Descent.ipynb).\n",
    "\n",
    "The dataset Apartment rental offers in Germany from kaggle (https://www.kaggle.com/datasets/corrieaar/apartment-rental-offers-in-germany) is used with the same parameters as the linear regression. The main difference with the plain linear regression is that CKKS encrypted tensors and operations are used to train the linear regression on encrypted data.\n",
    "\n",
    "Firstly, the libraries are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from time import time\n",
    "import psutil\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import tenseal as ts\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "In this section, the dataset is prepared for processing. The following actions are performed:\n",
    "\n",
    "- Amount of features reduced\n",
    "- Data converted to appropriate datatypes\n",
    "- Unsensical data is removed\n",
    "- NA values are removed\n",
    "- Outliers are removed as linear regression is sensitive to outliers\n",
    "- Categorical data is converted to numeric data\n",
    "- The data is split into a train and test set\n",
    "- The data is scaled as linear regression is sensitive to data ranges.\n",
    "\n",
    "The code leans on the exercises from the module ADML (https://gitlab.renku.hslu.ch/solange.emmenegger/ml-adml-hslu/-/tree/master/notebooks/03A%20Supervised%20Learning).## Data cleaning\n",
    "\n",
    "In this section, the dataset is prepared for processing. The following actions are performed:\n",
    "\n",
    "- Amount of features reduced\n",
    "- Data converted to appropriate datatypes\n",
    "- Unsensical data is removed\n",
    "- NA values are removed\n",
    "- Outliers are removed as linear regression is sensitive to outliers\n",
    "- Categorical data is converted to numeric data\n",
    "- The data is split into a train and test set\n",
    "- The data is scaled as linear regression is sensitive to data ranges.\n",
    "\n",
    "The code leans on the exercises from the module ADML (https://gitlab.renku.hslu.ch/solange.emmenegger/ml-adml-hslu/-/tree/master/notebooks/03A%20Supervised%20Learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"immo_data.csv\")\n",
    "\n",
    "columns_to_drop = ['serviceCharge', 'telekomTvOffer', 'noParkSpaces', 'pricetrend','regio3', 'heatingType', 'telekomUploadSpeed', 'telekomHybridUploadSpeed', 'lastRefurbish', 'newlyConst', 'picturecount', 'firingTypes', 'hasKitchen', 'geo_bln','cellar', 'yearConstructedRange', 'baseRent', 'houseNumber', 'geo_krs', 'interiorQual', 'petsAllowed', 'street', 'streetPlain', 'baseRentRange', 'geo_plz', 'thermalChar', 'floor', 'numberOfFloors', 'noRoomsRange', 'garden', 'livingSpaceRange', 'description', 'facilities', 'heatingCosts', 'energyEfficiencyClass', 'electricityBasePrice', 'electricityKwhPrice', 'date' ]\n",
    "df = df.drop(columns=columns_to_drop, axis=1)\n",
    "df['regio1'] = df.regio1.astype('category')\n",
    "df['regio2'] = df.regio1.astype('category')\n",
    "df['balcony'] = df['balcony'].astype(int)\n",
    "df['lift'] = df['lift'].astype(int)\n",
    "df['condition'] = df.condition.astype('category')\n",
    "df['typeOfFlat'] = df.typeOfFlat.astype('category')\n",
    "df = df.dropna()\n",
    "df = df[(df['totalRent'] != 0) & (df['livingSpace'] != 0) & (df['yearConstructed'] > 1940) & (df['yearConstructed'] < 2021)]\n",
    "numerical_cols = ['totalRent', 'yearConstructed', 'livingSpace', 'noRooms']\n",
    "# Remove outliers\n",
    "q3 = df.loc[:, numerical_cols].describe().loc['75%']\n",
    "iqr = q3 - df.loc[:, numerical_cols].describe().loc['25%']\n",
    "upper_boundary = q3 + 1.5*iqr\n",
    "upper_boundary\n",
    "\n",
    "df = df[(df.totalRent <= upper_boundary.totalRent) &\n",
    "        (df.yearConstructed <= upper_boundary.yearConstructed) &\n",
    "        (df.livingSpace <= upper_boundary.livingSpace) &\n",
    "         (df.noRooms <= upper_boundary.noRooms) ]\n",
    "\n",
    "df = pd.concat([df, pd.get_dummies(df.regio1)], axis='columns')\n",
    "df = pd.concat([df, pd.get_dummies(df.regio2)], axis='columns')\n",
    "df = pd.concat([df, pd.get_dummies(df.condition)], axis='columns')\n",
    "df = pd.concat([df, pd.get_dummies(df.typeOfFlat)], axis='columns')\n",
    "df.drop('regio1', axis='columns', inplace=True)\n",
    "df.drop('regio2', axis='columns', inplace=True)\n",
    "df.drop('condition', axis='columns', inplace=True)\n",
    "df.drop('typeOfFlat', axis='columns', inplace=True)\n",
    "df.drop(['scoutId'], axis='columns', inplace=True)\n",
    "\n",
    "train_rents, test_rents = train_test_split(df, test_size=0.4, random_state=42, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "train_rents = pd.DataFrame(scaler.fit_transform(train_rents), columns=train_rents.columns, index=train_rents.index)\n",
    "test_rents = pd.DataFrame(scaler.transform(test_rents), columns=test_rents.columns, index=test_rents.index)\n",
    "X_train_rents = train_rents.drop(columns=[\"totalRent\"]).values\n",
    "X_test_rents = test_rents.drop(columns=[\"totalRent\"]).values\n",
    "y_train_rents = train_rents.totalRent.values\n",
    "y_test_rents = test_rents.totalRent.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the TenSEAL context\n",
    "\n",
    "In order to use homomorphic encryption, Tenseal context with encryption parameters are defined. For the purposes of the training, we choose the polynomial degree of 8192, the coefficient modulous bitsizes of [40, 21, 21, 21, 21, 40] and the global scale as 21. The author identifies these criteria by testing the training, whether the TenSEAL library causes as error, because the cipher text starts to exceed the scale. The author chose the symmetric version of CKKS, as it has smaller ciphertexts and it computationally more effective. \n",
    "\n",
    "The polynomial modulus degree(poly_modulus_degree) has the following influence: (Taken from https://github.com/OpenMined/TenSEAL/blob/main/tutorials/Tutorial%202%20-%20Working%20with%20Approximate%20Numbers.ipynb, full reference below)\n",
    "- It changes the number of coefficients in the plaintext polynomials\n",
    "- It affects size of the ciphertext elements\n",
    "- The bigger it is, the worse the computation performance of the scheme is\n",
    "- The bigger it is, the better is the security level\n",
    "\n",
    "The coefficient modulus has an influence on: (Taken from https://github.com/OpenMined/TenSEAL/blob/main/tutorials/Tutorial%202%20-%20Working%20with%20Approximate%20Numbers.ipynb, full reference below)\n",
    "- the size of the ciphertext elements\n",
    "- The length of the list determines the number of encrypted multiplications supported\n",
    "- The bigger the modulus sizes are, the worse the security is \n",
    "\n",
    "The global scale is a scaling factor that affects the encoding prevision of the binary representation of numbers. (Taken from https://github.com/OpenMined/TenSEAL/blob/main/tutorials/Tutorial%202%20-%20Working%20with%20Approximate%20Numbers.ipynb, full reference below)\n",
    "\n",
    "The code is based as mentioned on the code from the ADML module from Hochschule Luzern as well as on the tutorials provided by the contibutors to the TenSEAL library (Ayoub Benaissa and Bilal Retiat and Bogdan Cebere and Alaa Eddine Belfedhal, TenSEAL: A Library for Encrypted Tensor Operations Using Homomorphic Encryption, 2021, 2104.03152m arXiV, visited at https://github.com/OpenMined/TenSEAL/tree/main/tutorials on 1 May 2024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context():\n",
    "    # parameters\n",
    "    poly_mod_degree = 8192\n",
    "    coeff_mod_bit_sizes = [40, 21, 21, 21, 21, 40]\n",
    "    enc_type = ts.ENCRYPTION_TYPE.ASYMMETRIC\n",
    "    # create TenSEALContext\n",
    "    context = ts.context(ts.SCHEME_TYPE.CKKS, poly_mod_degree, -1, coeff_mod_bit_sizes, encryption_type=enc_type)\n",
    "    context.global_scale = 2 ** 21\n",
    "    context.generate_galois_keys()\n",
    "    return context\n",
    "\n",
    "context = context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encrypt the data\n",
    "\n",
    "The following function encrypts the plaintext data, cleaned above, into CKKS tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt_trainset_tensor(X, y):    \n",
    "\n",
    "    enc_x_train = ts.ckks_tensor(context, X)\n",
    "    enc_y_train = ts.ckks_tensor(context, y)\n",
    "\n",
    "    return enc_x_train, enc_y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean squared error function\n",
    "\n",
    "The following function implements the mean squared error as with the plaintext model. It is used to measure the performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encrypted Mean squared error\n",
    "def cost(y, y_pred):\n",
    "    cost = np.sum(np.square(y - y_pred))/ (2 * len(y))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encrypted predict function\n",
    "\n",
    "The encrypted predict function performs the predict with linear regression on encrypted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_predict(enc_X, bias, thetas):\n",
    "    return enc_X.dot(thetas) + bias\n",
    "\n",
    "# The non encrypted predict is only used for measurement\n",
    "def predict(X, bias, thetas):\n",
    "    y_pred = bias + np.dot(X, thetas)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encrypted gradient function\n",
    "\n",
    "The function below implement the gradient calculation of the linear regression with encrypted operations. The N parameter is the reciprocal of the length of X. This is necessary as TenSEAL does not implement division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_gradient(X, y, bias, thetas, N):\n",
    "    diff = enc_predict(X, bias, thetas) - y\n",
    "\n",
    "    grad_bias = diff.sum() * N\n",
    "    grad_thetas = diff.dot(X) * N\n",
    "    return grad_bias, grad_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt(data, context):\n",
    "    return data.decrypt(context.secret_key()).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encrypted training function with minibatches\n",
    "\n",
    "The following function trains the parameters of the linear regression. Note that the thetas and bias are decrypted after each step in order to avoid deepening the ciphertext and increase its size. With the available resources in this project, decrypting the thetas and bias was attempted at the epoch level, however, the author does not possess enough RAM on this machine to perform the encrypted gradient function with thetas and bias as encrypted tensors.\n",
    "\n",
    "In a practical situation, the decryption of thetas and the bias below have to be send over the network back to the owner of the data, which holds the secret key. The owner of the data thereafter send the unencrypted thetas and bias back to the owner of the model in order to proceed with the training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent training\n",
    "\n",
    "The next function performs the gradient descent training. It is build on the code from the plain linear regression, with the addition that it reads batches from the encrypted binary dataset and uses it in gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_fit_minibatch(X_train, y_train, alpha, num_epochs, reciprocal_N, batch_size, display_every=50):\n",
    "    number_features = X_train.shape[1]\n",
    "    bias = 0.0\n",
    "    thetas = np.random.randn(*(1, number_features )).reshape(-1)\n",
    "    num_samples = len(X_train)\n",
    "    indices_train = np.arange(len(X_train))\n",
    "    \n",
    "    steps = int(num_samples/batch_size)\n",
    "\n",
    "    hist = defaultdict(list)\n",
    "    for epoch in tqdm(range(1, num_epochs+1)):\n",
    "        np.random.shuffle(indices_train)\n",
    "        \n",
    "        X_train_epoch = X_train[indices_train]\n",
    "        y_train_epoch = y_train[indices_train]\n",
    "\n",
    "        for step in range(steps):\n",
    "            start = step * batch_size\n",
    "            end = step * batch_size + batch_size\n",
    "            # Slide the data into mini batches\n",
    "            X_train_mini = X_train_epoch[start:end]\n",
    "            y_train_mini = y_train_epoch[start:end]\n",
    "            # encrypt the minibatch, this would take place at the owner of the data and then may be transferred over the network. Due to the focus on the computation overhead and RAM requirements, the data is encrypted during training.\n",
    "            enc_x_train, enc_y_train = encrypt_trainset_tensor(X_train_mini, y_train_mini)\n",
    "\n",
    "            grad_bias, grad_thetas = enc_gradient(enc_x_train, enc_y_train, bias, thetas, reciprocal_N)\n",
    "            # The decryption of the grad_bias and grad_thetas has to be performed at the owner of the data who has the symmetric key. \n",
    "            # Not decrypting the data would require to deepen the cipher and significatly increase the ciphertext size, requiring significantly more computation resources.\n",
    "            bias = bias - alpha * np.array(grad_bias.decrypt().tolist())\n",
    "            thetas = thetas - alpha * np.array(grad_thetas.decrypt().tolist())\n",
    "\n",
    "        # This part of the code servers only to log the performance during each epoch and is not necessary in a practical situation\n",
    "        y_pred_train = predict(X_train_epoch, bias, thetas)\n",
    "        train_cost = np.array(cost(y_train_epoch, y_pred_train))\n",
    "        train_r2 = np.array(r2_score(y_train_epoch, y_pred_train))\n",
    "        \n",
    "        hist[\"train_cost\"].append(train_cost)\n",
    "        hist[\"train_r2\"].append(train_r2)\n",
    "        \n",
    "        if epoch % display_every == 0:\n",
    "            print(\"Epoch {0} - cost: {1:.2} - r2: {2:.4}\"\n",
    "                .format(epoch, train_cost, train_r2))\n",
    "        \n",
    "    return bias, thetas, hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the dataset is sliced in order to prevent the training to run for days. The size of 10000 data samples has been chosen to perform the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f675b773a4b54ee79046c6556d975a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training of the Linear Regression took 1042 seconds\n",
      "CPU usage difference: 35.7%\n",
      "Memory usage difference: 10164625408 bytes\n"
     ]
    }
   ],
   "source": [
    "len_taken = 10000\n",
    "batch_size = 100\n",
    "reciprocal_N = 1 / batch_size\n",
    "\n",
    "alpha = 0.01\n",
    "num_epochs = 6\n",
    "\n",
    "# measure time\n",
    "t_start = time()\n",
    "# measure resource usage\n",
    "cpu_percent = psutil.cpu_percent()\n",
    "mem_usage = psutil.Process().memory_info().rss\n",
    "enc_bias, enc_thetas, hist_rent_enc = enc_fit_minibatch(X_train_rents[:len_taken], y_train_rents[:len_taken], alpha, num_epochs, reciprocal_N, batch_size)\n",
    "cpu_percent_end = psutil.cpu_percent()\n",
    "mem_usage_end = psutil.Process().memory_info().rss\n",
    "\n",
    "# Calculate the differences\n",
    "cpu_diff = cpu_percent_end - cpu_percent\n",
    "mem_diff = mem_usage_end - mem_usage\n",
    "t_end = time()\n",
    "print(f\"Training of the Linear Regression took {int(t_end - t_start)} seconds\")\n",
    "print(f\"CPU usage difference: {cpu_diff}%\")\n",
    "print(f\"Memory usage difference: {mem_diff} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the helper functions for the visualiation of the data are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_validation_curve(data, label, ax=None, ylim=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_title(\"Validation Curve\")\n",
    "        ax.set_ylabel(\"Cost\")\n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(ylim)\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.plot(data, label=label)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "def plot_validation_curves(hist, hist_enc, ylim=None):\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(16,5))\n",
    "\n",
    "    ax[0].set_title(\"Train Cost\")\n",
    "    ax[0].set_ylabel(\"Cost\")\n",
    "    ax[0].set_ylim(0, 1.76)  # Set y-axis limits \n",
    "    ax[0].set_yticks(np.arange(0, 1.76, 0.25))  # Set y-axis ticks\n",
    "    plot_validation_curve(hist[\"train_cost\"], label=\"Plaintext\", ax=ax[0], ylim=ylim)\n",
    "    plot_validation_curve(hist_enc[\"train_cost\"], label=\"Encrypted\", ax=ax[0], ylim=ylim)\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].set_title(\"Train R2\")\n",
    "    ax[1].set_ylabel(\"R2\")\n",
    "    ax[1].set_ylim(-1, 1)\n",
    "    plot_validation_curve(hist[\"train_r2\"], label=\"Plaintext\", ax=ax[1])\n",
    "    plot_validation_curve(hist_enc[\"train_r2\"], label=\"Encrypted\", ax=ax[1])\n",
    "    ax[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist_rent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_validation_curves(\u001b[43mhist_rent\u001b[49m, hist_rent_enc)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hist_rent' is not defined"
     ]
    }
   ],
   "source": [
    "plot_validation_curves(hist_rent, hist_rent_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the trained encrypted linear regression is evaluated on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.7264176849345279\n"
     ]
    }
   ],
   "source": [
    "y_pred_rents = predict(X_test_rents, enc_bias, enc_thetas)\n",
    "r2 = r2_score(y_test_rents, y_pred_rents)\n",
    "print(\"R2:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the linear regression model with data read from a file (not evaluated)\n",
    "\n",
    "To provide a glimpse into the case, how long a linear regression model would be train while reading the encrypted data from a file, modified functions are presented that enable this. Again, this is not the proof of concept subject to evaluation.\n",
    "\n",
    "Firstly, the context is defined. Thereafter, a public context is created by removing the secret key and writing it to a binary file. This binary file would be provided to MLco in the hypothetical scenario. The code is based on the tests from and TenSEAL library (Ayoub Benaissa and Bilal Retiat and Bogdan Cebere and Alaa Eddine Belfedhal, TenSEAL: A Library for Encrypted Tensor Operations Using Homomorphic Encryption, 2021, 2104.03152m arXiV, visited on 18 May 2024 at https://github.com/OpenMined/TenSEAL/blob/main/tests/python/tenseal/tensors/test_serialization.py) and suggestions from ChatGPT (“How can I efficiently write serialized CKKS vectors into a file?”, ChatGPT (GPT-4o), OpenAI, generated on 18 May 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context():\n",
    "    # parameters\n",
    "    poly_mod_degree = 8192\n",
    "    coeff_mod_bit_sizes = [40, 21, 21, 21, 21, 40]\n",
    "    enc_type = ts.ENCRYPTION_TYPE.ASYMMETRIC\n",
    "    # create TenSEALContext\n",
    "    context = ts.context(ts.SCHEME_TYPE.CKKS, poly_mod_degree, -1, coeff_mod_bit_sizes, encryption_type=enc_type)\n",
    "    context.global_scale = 2 ** 21\n",
    "    context.generate_galois_keys()\n",
    "    return context\n",
    "\n",
    "# save the context with the secret key to keep it persistent. This has to be hidden at \n",
    "context = context()\n",
    "public_context = context.copy()\n",
    "public_context.make_context_public()\n",
    "\n",
    "with open(\"public_context.bin\", 'wb') as pub_file:\n",
    "    pub_file.write(public_context.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encrypt data and write it in batches to a binary file function\n",
    "\n",
    "The following function encrypt the data in batches and then writes into a binary file. This is the encrypted data that is provided to MLco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt_and_serialize_batches(data_X, data_y, batch_size, filename, context):\n",
    "    num_samples = data_X.shape[0]\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    data_X = data_X[indices]\n",
    "    data_y = data_y[indices]\n",
    "    \n",
    "    with open(filename, 'wb') as file:\n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            end = min(start + batch_size, num_samples)\n",
    "            data_batch_X = data_X[start:end]\n",
    "            data_batch_y = data_y[start:end]\n",
    "            \n",
    "            # Encrypt the batch\n",
    "            enc_train_X = ts.ckks_tensor(context, data_batch_X)\n",
    "            enc_train_y = ts.ckks_tensor(context, data_batch_y)\n",
    "            \n",
    "            # Serialize the encrypted tensors\n",
    "            serialized_batch_X = enc_train_X.serialize()\n",
    "            serialized_batch_y = enc_train_y.serialize()\n",
    "            \n",
    "            # Write the length of the serialized data and the data itself\n",
    "            file.write(len(serialized_batch_X).to_bytes(4, byteorder='big'))\n",
    "            file.write(serialized_batch_X)\n",
    "            file.write(len(serialized_batch_y).to_bytes(4, byteorder='big'))\n",
    "            file.write(serialized_batch_y)\n",
    "            \n",
    "            # Debug: Print lengths written to file\n",
    "            # print(f\"Batch start: {start}, Batch end: {end} Serialized X length: {len(serialized_batch_X)}, Serialized y length: {len(serialized_batch_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and decrypt data function\n",
    "\n",
    "deserialize_decrypt_next_batch function reads the batches from the file one at a time and returns them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_decrypt_next_batch(file, context):\n",
    "    len_x_bytes = file.read(4)\n",
    "    if not len_x_bytes:\n",
    "        return None, None\n",
    "\n",
    "    len_x = int.from_bytes(len_x_bytes, byteorder='big')\n",
    "    serialized_batch_X = file.read(len_x)\n",
    "    \n",
    "    len_y_bytes = file.read(4)\n",
    "    len_y = int.from_bytes(len_y_bytes, byteorder='big')\n",
    "    serialized_batch_y = file.read(len_y)\n",
    "    \n",
    "    enc_train_X = ts.ckks_tensor_from(context, serialized_batch_X)\n",
    "    enc_train_y = ts.ckks_tensor_from(context, serialized_batch_y)\n",
    "    \n",
    "    return enc_train_X, enc_train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encryption of the data\n",
    "\n",
    "Before providing the data to MLco, the data is encrypted by SRE. WARNING, this will create a file of 170 GB size. The data and the public context is then provided to MLco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encrypt_and_serialize_batches(X_train_rents[:100], y_train_rents[:100], 100, \"enc_dataframe.bin\", context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the context for the data owner and the data user\n",
    "\n",
    "Data owner is SRE from the example. SRE saves its context with the secret key to be able to decrypt the data and the results of operations performed on the data.\n",
    "\n",
    "The data user us MLco. MLco only receives the public context that only contains the public key. The public context is necessary to reestablish the CKKS vectors from the binary file as packed by SRE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"public_context.bin\", 'rb') as pub_file:\n",
    "    serialized_public_context = pub_file.read()\n",
    "\n",
    "public_context = ts.context_from(serialized_public_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified gradient descent function\n",
    "\n",
    "Below, we modify the gradient descent function to read batches from the a binary file instead of encrypting the data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_fit_minibatch_readfile(X_train, y_train, alpha, num_epochs, batch_size, file, public_context, context, display_every=1):\n",
    "    number_features = X_train.shape[1]\n",
    "    bias = 0.0\n",
    "    thetas = np.random.randn(*(1, number_features )).reshape(-1)\n",
    "    reciprocal_N = 1 / batch_size\n",
    "    \n",
    "    hist = defaultdict(list)\n",
    "\n",
    "    for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "        with open(file, 'rb') as f:\n",
    "            while True:\n",
    "                X_train_iteration, y_train_iteration = deserialize_decrypt_next_batch(f, public_context)\n",
    "                if X_train_iteration is None:\n",
    "                    break\n",
    "                grad_bias, grad_thetas = enc_gradient(X_train_iteration, y_train_iteration, bias, thetas, reciprocal_N)\n",
    "                \n",
    "                # Decrypt the gradients (occurs at the data owner, SRE as MLco does not have the secret key)\n",
    "                decrypted_grad_bias = np.array(decrypt(grad_bias, context))\n",
    "                decrypted_grad_thetas = np.array(decrypt(grad_thetas, context))\n",
    "                \n",
    "                # Update parameters\n",
    "                bias = bias - alpha * decrypted_grad_bias\n",
    "                thetas = thetas - alpha * decrypted_grad_thetas\n",
    "\n",
    "        # Log performance\n",
    "        y_pred_train = predict(X_train, bias, thetas)\n",
    "        train_cost = cost(y_train, y_pred_train)\n",
    "        train_r2 = r2_score(y_train, y_pred_train)\n",
    "        \n",
    "        hist[\"train_cost\"].append(train_cost)\n",
    "        hist[\"train_r2\"].append(train_r2)\n",
    "        \n",
    "        if epoch % display_every == 0:\n",
    "            print(f\"Epoch {epoch} - cost: {train_cost:.2f} - r2: {train_r2:.4f}\")\n",
    "        \n",
    "    return bias, thetas, hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the training of the linear regression with reading from the file is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3bce9dbc96240b3be909b39de028067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - cost: 17.33 - r2: -38.7215\n",
      "Epoch 2 - cost: 15.89 - r2: -35.4295\n",
      "Epoch 3 - cost: 14.61 - r2: -32.4869\n",
      "Epoch 4 - cost: 13.46 - r2: -29.8521\n",
      "Epoch 5 - cost: 12.43 - r2: -27.4878\n",
      "Training of the Linear Regression took 332 seconds\n",
      "CPU usage difference: 6.9%\n",
      "Memory usage difference: 12310495232 bytes\n"
     ]
    }
   ],
   "source": [
    "len_taken = 10000\n",
    "batch_size = 100\n",
    "\n",
    "alpha = 0.01\n",
    "num_epochs = 6\n",
    "\n",
    "# measure time\n",
    "t_start = time()\n",
    "# measure resource usage\n",
    "cpu_percent = psutil.cpu_percent()\n",
    "mem_usage = psutil.Process().memory_info().rss\n",
    "enc_bias, enc_thetas, hist_rent_enc = enc_fit_minibatch_readfile(X_train_rents[:len_taken], y_train_rents[:len_taken], alpha, num_epochs, batch_size, \"enc_dataframe.bin\", public_context, context)\n",
    "cpu_percent_end = psutil.cpu_percent()\n",
    "mem_usage_end = psutil.Process().memory_info().rss\n",
    "\n",
    "# Calculate the differences\n",
    "cpu_diff = cpu_percent_end - cpu_percent\n",
    "mem_diff = mem_usage_end - mem_usage\n",
    "t_end = time()\n",
    "print(f\"Training of the Linear Regression took {int(t_end - t_start)} seconds\")\n",
    "print(f\"CPU usage difference: {cpu_diff}%\")\n",
    "print(f\"Memory usage difference: {mem_diff} bytes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
